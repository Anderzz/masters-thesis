{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hallo fra datasettet!\n",
      "Number of recordings in dataset:1600\n",
      "Transforms:\n",
      "    Normalize\n",
      "    ToTensorV2\n",
      "torch.Size([256, 256])\n",
      "torch.Size([1, 256, 256])\n",
      "tensor(-0.9013) tensor(-0.8996)\n",
      "tensor(0.) tensor(3.)\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "import utils\n",
    "import albumentations as A\n",
    "from albumentations.pytorch import ToTensorV2\n",
    "\n",
    "\n",
    "class Labeled_dataset(torch.utils.data.Dataset):\n",
    "    \"Characterizes a dataset for labeled segmentation dataset\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        patient_list,\n",
    "        input_dir,\n",
    "        augmentation_params=None,\n",
    "        transform=None,\n",
    "        verbose=True,\n",
    "        return_file_loc=False,\n",
    "    ):\n",
    "        \"\"\"\n",
    "        :param patient_list: list or set of patient ids to include in dataset\n",
    "        :param input_dir: directory where data is stored. This directory should contain a folder for each patient,\n",
    "                            which in turn contains one file for each recording. Each file should be a .npy file\n",
    "                            containing a tuple of the ultrasound image and its segmentation mask as a numpy arrays.\n",
    "        :param augmentation_params: list of augmentation parameters\n",
    "        :param verbose: whether to print information about dataset\n",
    "        :param return_file_loc: whether to return file location of sample when loading data\n",
    "        \"\"\"\n",
    "        self.verbose = verbose\n",
    "        self.patient_list = patient_list\n",
    "        self.input_dir = input_dir\n",
    "        self.init_index_to_file_dict()\n",
    "        self.return_file_loc = return_file_loc\n",
    "        self.transform = transform\n",
    "        if augmentation_params is None:\n",
    "            self.augmentations = None\n",
    "        else:\n",
    "            self.augmentations = augmentations.get_augmentation_funcs(\n",
    "                augmentation_params\n",
    "            )\n",
    "        # list the transforms\n",
    "        if self.transform is not None:\n",
    "            print(\"Transforms:\")\n",
    "            for transform in self.transform:\n",
    "                print(f\"    {transform.__class__.__name__}\")\n",
    "\n",
    "    def __len__(self):\n",
    "        \"Denotes the total number of patients\"\n",
    "        return len(self.list_IDs)\n",
    "\n",
    "    def __getitem__(self, custom_index=None):\n",
    "        \"\"\"\n",
    "        'Generates one sample of data'\n",
    "        :param custom_index: index of sample to load. If None, a random sample will be loaded\n",
    "        :return: X,y, the sample (ultrasound image) and its label (segmentation mask) as 2D numpy arrays\n",
    "        \"\"\"\n",
    "        if custom_index is None:\n",
    "            index = self.index\n",
    "        else:\n",
    "            index = custom_index\n",
    "        # Generate data\n",
    "        if self.return_file_loc:\n",
    "            X, y, file_loc = self.__data_generation(index)\n",
    "        else:\n",
    "            X, y = self.__data_generation(index)\n",
    "        y = np.squeeze(y)\n",
    "\n",
    "        if self.augmentations is not None:\n",
    "            X, y = augmentations.apply_augmentations((X, y), self.augmentations)\n",
    "\n",
    "        # this is where the actual augmentation happens\n",
    "        if self.transform is not None:\n",
    "            transformed = self.transform(image=X, mask=y)\n",
    "            X = transformed[\"image\"]\n",
    "            y = transformed[\"mask\"]\n",
    "        # if isinstance(X, list):\n",
    "        #     X = [torch.from_numpy(x) for x in X]\n",
    "        # else:\n",
    "        #     X = torch.from_numpy(X)\n",
    "\n",
    "        if self.return_file_loc:\n",
    "            return X, y, file_loc\n",
    "        else:\n",
    "            return X, y\n",
    "\n",
    "    def __data_generation(self, index):\n",
    "        \"Generates data containing batch_size samples\"  # X : (n_samples, *dim)\n",
    "        # Generate data\n",
    "        file_loc = self.index_to_file_dict[index]\n",
    "        X, y = np.load(file_loc, allow_pickle=True)\n",
    "        X = X.astype(np.float32)\n",
    "        y = y.astype(np.float32)\n",
    "        if self.return_file_loc:\n",
    "            return X, y, file_loc\n",
    "        else:\n",
    "            return X, y\n",
    "\n",
    "    def init_index_to_file_dict(self):\n",
    "        \"\"\"\n",
    "        Initializes a dictionary mapping index to file location. The index is used by the pytorch dataloader to\n",
    "        load samples. The file location is used to retrieve the file location of a sample when loading data.\n",
    "        The function sets the following attributes:\n",
    "            - self.index_to_file_dict: dictionary mapping index to file location\n",
    "            - self.list_IDs: list of indices\n",
    "        \"\"\"\n",
    "        self.index_to_file_dict = {}\n",
    "        index = 0\n",
    "        for patient_id in os.listdir(self.input_dir):\n",
    "            if patient_id in self.patient_list:\n",
    "                patient_dir = os.path.join(self.input_dir, patient_id)\n",
    "                for recording in os.listdir(patient_dir):\n",
    "                    recording_path = os.path.join(patient_dir, recording)\n",
    "                    self.index_to_file_dict[index] = recording_path\n",
    "                    index += 1\n",
    "        if self.verbose:\n",
    "            print(\"Number of recordings in dataset:\" + str(index))\n",
    "        self.list_IDs = np.arange(index)\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    import augmentations\n",
    "\n",
    "    print(\"hallo fra datasettet!\")\n",
    "    # example usage\n",
    "    input_dir = \"../preprocessing_output/cv1/train_val\"\n",
    "    splits = utils.get_splits(1, \"../../local_data/subgroups_CAMUS\")\n",
    "    train_set, val_set, test_set = splits\n",
    "    params = {\"batch_size\": 1, \"shuffle\": True, \"num_workers\": 1}\n",
    "    augmentation_params = []\n",
    "    means = []\n",
    "    stds = []\n",
    "    train_transform = A.Compose(\n",
    "        [\n",
    "            # A.ShiftScaleRotate(\n",
    "            #     shift_limit=0.2, scale_limit=0.2, rotate_limit=10, p=0.5\n",
    "            # ),\n",
    "            # A.RandomGamma(gamma_limit=(80, 120), p=0.5),\n",
    "            # A.GaussNoise(var_limit=(10.0, 50.0), p=0.5),\n",
    "            # A.ImageCompression(quality_lower=60, quality_upper=100, p=0.5),\n",
    "            # A.RandomBrightnessContrast(brightness_limit=0.3, contrast_limit=0.3, p=0.5),\n",
    "            # A.Normalize(mean=(0.485), std=(0.229)),\n",
    "            A.Normalize(mean=(48.6671), std=(53.9987), max_pixel_value=1.0),\n",
    "            ToTensorV2(),\n",
    "        ]\n",
    "    )\n",
    "    dataset_train_pytorch = Labeled_dataset(\n",
    "        train_set,\n",
    "        input_dir,\n",
    "        augmentation_params=augmentation_params,\n",
    "        verbose=True,\n",
    "        transform=train_transform,\n",
    "    )\n",
    "    dataloader = torch.utils.data.DataLoader(dataset_train_pytorch, **params)\n",
    "    train_iterator_pytorch = iter(dataloader)\n",
    "    X, y = next(train_iterator_pytorch)\n",
    "    X = X.squeeze()\n",
    "    print(X.shape)\n",
    "    print(y.shape)\n",
    "    print(X[0].min(), X[0].max())\n",
    "    print(y[0].min(), y[0].max())\n",
    "\n",
    "    # find mean and std for entire dataset\n",
    "    # for i in range(len(dataset_train_pytorch)):\n",
    "    #     X, y = dataset_train_pytorch[i]\n",
    "    #     means.append(X.mean().item())\n",
    "    #     stds.append(X.std().item())\n",
    "\n",
    "    # plotting some samples\n",
    "    # for i in range(3):\n",
    "    #     X, y = next(train_iterator_pytorch)\n",
    "    #     X = X.squeeze()\n",
    "    #     plt.imshow(X[0].T, cmap=\"gray\")\n",
    "    #     plt.savefig(\"test.png\")\n",
    "    #     plt.show()\n",
    "    #     plt.imshow(y[0].T, cmap=\"gray\")\n",
    "    #     plt.show()\n",
    "    #     visualization = utils.create_visualization(X[0].T, y[0].T)\n",
    "    #     plt.imshow(visualization)\n",
    "    #     plt.show()\n",
    "    #     plt.clf()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(48.667148208618165, 53.99869374036789)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean(means), np.mean(stds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
